{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afeedc12-2bf9-40d4-a9c4-a4bb4399bf5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bded9b7-56fe-476f-93d5-fe0bd14170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import os\n",
    "import math\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import trimesh\n",
    "import logging\n",
    "import itertools\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Conv2D, MaxPooling2D\n",
    "\n",
    "# increase matplotlib plots font size\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# dataset root path\n",
    "DATASET_ROOT = '/run/media/rr/M2/DevOps/jupyter-lab/CIDL/dataset/'\n",
    "\n",
    "# final preprocessed dataset directory path\n",
    "DATASET_PATH = os.path.join(DATASET_ROOT, 'Preprocessed')\n",
    "\n",
    "# directory where to save the best model for each fold\n",
    "saved_models = 'saved_models'\n",
    "images_models_save_dir = os.path.join(saved_models, 'images')\n",
    "pointclouds_models_save_dir = os.path.join(saved_models, 'pointclouds')\n",
    "\n",
    "# needed to create pointclouds dataset\n",
    "#class_labels_dict = {'table':0, 'chair':1, 'lamp':2, 'dresser':3, 'sofa':4}\n",
    "class_labels_dict = {'chair': 0, 'dresser': 1, 'lamp': 2, 'sofa': 3, 'table': 4} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b543ec-083a-4573-bc2b-39f0eccdfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create saved_models directory if not present\n",
    "if not os.path.isdir(saved_models):\n",
    "    os.mkdir(saved_models)\n",
    "    \n",
    "if not os.path.isdir(images_models_save_dir):\n",
    "    os.mkdir(images_models_save_dir)\n",
    "\n",
    "if not os.path.isdir(pointclouds_models_save_dir):\n",
    "    os.mkdir(pointclouds_models_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f62597b-f41a-48fd-90ad-3904432f1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to get model name\n",
    "def get_model_name(model_name, fold_counter):\n",
    "    return model_name + '-fold-' + str(fold_counter) + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d2d156-539f-4f3f-85bd-acbe9dec8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and acuracy for each epoch\n",
    "def plot_train_loss_accuracy(save_path, train_loss, train_accuracy, val_loss, val_accuracy):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(train_loss,'g-', label=\"Training Set\")\n",
    "    plt.plot(val_loss,'r-', label=\"Validation Set\")\n",
    "    plt.title('Training and Validation Sets Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(save_path + '-train-val-loss.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(train_accuracy,'g-', label=\"Training Set\")\n",
    "    plt.plot(val_accuracy,'r-', label=\"Validation Set\")\n",
    "    plt.title('Training and Validation Sets Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(save_path + '-train-val-accuracy.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ccd762-98a3-4f5a-b929-b57c8ea33687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for scoring roc auc score for multi-class\n",
    "def multiclass_roc_auc_score(save_path, y_test, y_pred, average=\"macro\"):\n",
    "    # target classes\n",
    "    target = ['Chair', 'Dresser', 'Lamp', 'Sofa', 'Table']\n",
    "\n",
    "    # Binarize labels in a one-vs-all fashion\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    # ROC curve plot\n",
    "    fig, c_ax = plt.subplots(1,1, figsize=(12, 8))\n",
    "\n",
    "    for (idx, c_label) in enumerate(target):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label='Random Guessing')\n",
    "\n",
    "    # plot legend and axis labels\n",
    "    c_ax.legend()\n",
    "    c_ax.set_xlabel('False Positive Rate')\n",
    "    c_ax.set_ylabel('True Positive Rate')\n",
    "    plt.title(\"ROC Curves per Class\")\n",
    "    plt.savefig(save_path + '-ROC.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95378c68-caaf-4917-a2ff-66c626da2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function used to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, save_path, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = numpy.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "        cm = numpy.around(cm, decimals=2)\n",
    "        cm[numpy.isnan(cm)] = 0.0\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path + '-confusion-matrix.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d158d42a-301f-4d43-b7b2-7a2cc737ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to augment pointclouds data\n",
    "def augment(points, label):\n",
    "    # jitter points\n",
    "    points += tensorflow.random.uniform(points.shape, -0.005, 0.005, dtype=tensorflow.float64)\n",
    "    # shuffle points\n",
    "    points = tensorflow.random.shuffle(points)\n",
    "    return points, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3186d841-5e1d-4237-8a25-f49cc0f3115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only log critical messages\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "290f4fbd-457c-457e-8ab7-df7b8f31b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images csv file\n",
    "images_data = pandas.read_csv(os.path.join(DATASET_PATH, 'images.csv'))\n",
    "\n",
    "# extract images path and class labels\n",
    "images_X = images_data[['filename']]\n",
    "images_Y = images_data[['class_label']]\n",
    "\n",
    "# load pointclouds csv file\n",
    "pointclouds_data = pandas.read_csv(os.path.join(DATASET_PATH, 'pointclouds.csv'))\n",
    "\n",
    "# extract images path and class labels\n",
    "pointclouds_X = pointclouds_data[['filename']]\n",
    "pointclouds_Y = pointclouds_data[['class_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f15531-b289-41a7-8b18-20e12c88676e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Stratified K-Fold Cross Validation for images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b278d65c-c6e6-4851-81af-126868ae2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateDecay:\n",
    "    def plot(self, save_path, epochs, title=\"Learning Rate Schedule\"):\n",
    "        # compute the set of learning rates for each corresponding epoch\n",
    "        lrs = [self(i) for i in epochs]\n",
    "\n",
    "        # the learning rate schedule\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(epochs, lrs)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.savefig(save_path + '-lr-decay.jpg', bbox_inches='tight', dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "class StepDecay(LearningRateDecay):\n",
    "    def __init__(self, initAlpha=0.001, factor=0.6, dropEvery=20):\n",
    "        # store the base initial learning rate, drop factor, and\n",
    "        # epochs to drop every\n",
    "        self.initAlpha = initAlpha\n",
    "        self.factor = factor\n",
    "        self.dropEvery = dropEvery\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        # compute the learning rate for the current epoch\n",
    "        exp = numpy.floor((1 + epoch) / self.dropEvery)\n",
    "        alpha = self.initAlpha * (self.factor ** exp)\n",
    "\n",
    "        # return the learning rate\n",
    "        return float(alpha)\n",
    "\n",
    "class PolynomialDecay(LearningRateDecay):\n",
    "    def __init__(self, maxEpochs=100, initAlpha=0.01, power=1.0):\n",
    "        # store the maximum number of epochs, base learning rate,\n",
    "        # and power of the polynomial\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.initAlpha = initAlpha\n",
    "        self.power = power\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        # compute the new learning rate based on polynomial decay\n",
    "        decay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
    "        alpha = self.initAlpha * decay\n",
    "\n",
    "        # return the new learning rate\n",
    "        return float(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27fb91c7-8105-4d85-86c7-c53308967008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_kfold_validation_model(model_name, n_splits, test_size, shuffle, model, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global images_data\n",
    "    global images_X\n",
    "    global images_Y\n",
    "    if resample_data > 0:\n",
    "        images_data = images_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        images_X = images_data[['filename']]\n",
    "        images_Y = images_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "\n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "\n",
    "    # split train and test dataset\n",
    "    images_train, images_test = train_test_split(images_data, test_size=test_size, stratify=images_Y)\n",
    "    images_train_X = images_train[['filename']]\n",
    "    images_train_Y = images_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # instanciate image generator with data augmentation if required\n",
    "    if augment == True:\n",
    "        train_image_data_generator = ImageDataGenerator(rotation_range=40, width_shift_range=0.01,\n",
    "                                                  height_shift_range=0.01, horizontal_flip=True,\n",
    "                                                  shear_range=20.0, fill_mode='nearest')\n",
    "    else:\n",
    "        train_image_data_generator = ImageDataGenerator()\n",
    "\n",
    "    # test split\n",
    "    test_image_data_generator = ImageDataGenerator()\n",
    "    test_data_generator  = test_image_data_generator.flow_from_dataframe(images_test, directory=None,\n",
    "                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                        class_mode=\"categorical\", shuffle=False,\n",
    "                                                        target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "    # generate training and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(images_train_X, images_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(images_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(images_models_save_dir, model_name))\n",
    "        images_model_save_path = os.path.join(images_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = images_train.iloc[train_index]\n",
    "        validation_data = images_train.iloc[validation_index]\n",
    "        train_data_generator = train_image_data_generator.flow_from_dataframe(training_data, directory=None,\n",
    "                                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                                        class_mode=\"categorical\", shuffle=shuffle,\n",
    "                                                                        target_size=target_size, batch_size=batch_size)\n",
    "        validation_image_data_generator = ImageDataGenerator()\n",
    "        validation_data_generator = validation_image_data_generator.flow_from_dataframe(validation_data, directory=None,\n",
    "                                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                                        class_mode=\"categorical\", shuffle=False,\n",
    "                                                                        target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(images_model_save_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "        val_accuracy_early_stopping = tensorflow.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.001, patience=50, verbose=1)\n",
    "        callbacks_list = [best_model_checkpoint, val_accuracy_early_stopping]\n",
    "\n",
    "        # compile model\n",
    "        model.summary()\n",
    "        model.compile(tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate, decay=decay), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_data_generator,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_data_generator)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(images_model_save_path, history.history[\"loss\"], history.history[\"accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model\n",
    "        model.load_weights(images_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_data_generator.reset()\n",
    "        test_loss, test_accuracy = model.evaluate(test_data_generator, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # --- Report ---\n",
    "        target_names = []\n",
    "        for key in test_data_generator.class_indices:\n",
    "            target_names.append(key)\n",
    "\n",
    "        # Confution Matrix\n",
    "        Y_pred = model.predict(test_data_generator)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=-1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_data_generator.classes, y_pred)\n",
    "        plot_confusion_matrix(cm, target_names, images_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_data_generator.classes, y_pred, target_names=target_names))\n",
    "        \n",
    "        # Plot ROC curve per class and print AUC score\n",
    "        print('ROC AUC score:', multiclass_roc_auc_score(images_model_save_path, test_data_generator.classes, y_pred))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold == True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931a65d-a74e-4159-b956-f842e737b0d3",
   "metadata": {},
   "source": [
    "#### **Auxiliary method to be used when passing layers array instead of the full model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2114563-cabb-40e4-928d-bf56b801c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_kfold_validation_layers(model_name, n_splits, test_size, shuffle, layers, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global images_data\n",
    "    global images_X\n",
    "    global images_Y\n",
    "    if resample_data > 0:\n",
    "        images_data = images_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        images_X = images_data[['filename']]\n",
    "        images_Y = images_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "\n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "\n",
    "    # split train and test dataset\n",
    "    images_train, images_test = train_test_split(images_data, test_size=test_size, stratify=images_Y)\n",
    "    images_train_X = images_train[['filename']]\n",
    "    images_train_Y = images_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # instanciate image generator with data augmentation if required\n",
    "    if augment == True:\n",
    "        train_image_data_generator = ImageDataGenerator(rotation_range=40, width_shift_range=0.01,\n",
    "                                                  height_shift_range=0.01, horizontal_flip=True,\n",
    "                                                  shear_range=20.0, fill_mode='nearest', rescale=1./255)\n",
    "    else:\n",
    "        train_image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # test split\n",
    "    test_image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "    test_data_generator  = test_image_data_generator.flow_from_dataframe(images_test, directory=None,\n",
    "                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                        class_mode=\"categorical\", shuffle=False,\n",
    "                                                        target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "    # generate training and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(images_train_X, images_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(images_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(images_models_save_dir, model_name))\n",
    "        images_model_save_path = os.path.join(images_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = images_train.iloc[train_index]\n",
    "        validation_data = images_train.iloc[validation_index]\n",
    "        train_data_generator = train_image_data_generator.flow_from_dataframe(training_data, directory=None,\n",
    "                                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                                        class_mode=\"categorical\", shuffle=shuffle,\n",
    "                                                                        target_size=target_size, batch_size=batch_size)\n",
    "        validation_image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "        validation_data_generator = validation_image_data_generator.flow_from_dataframe(validation_data, directory=None,\n",
    "                                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                                        class_mode=\"categorical\", shuffle=False,\n",
    "                                                                        target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "        # learning rate\n",
    "        step_decay_schedule = StepDecay(initAlpha=learning_rate, factor=0.9, dropEvery=30)\n",
    "        lr_scheduler_callback = tensorflow.keras.callbacks.LearningRateScheduler(step_decay_schedule)\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(images_model_save_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "        val_accuracy_early_stopping = tensorflow.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.001, patience=50, verbose=1)\n",
    "        callbacks_list = [best_model_checkpoint, val_accuracy_early_stopping, lr_scheduler_callback]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model = models.Sequential(name=model_name + \"-\" + str(fold_counter))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.summary()\n",
    "        model.compile(tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate, decay=decay), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_data_generator,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_data_generator)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(images_model_save_path, history.history[\"loss\"], history.history[\"accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model\n",
    "        model.load_weights(images_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_data_generator.reset()\n",
    "        test_loss, test_accuracy = model.evaluate(test_data_generator, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # --- Report ---\n",
    "        target_names = []\n",
    "        for key in test_data_generator.class_indices:\n",
    "            target_names.append(key)\n",
    "\n",
    "        # Confution Matrix\n",
    "        Y_pred = model.predict(test_data_generator)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=-1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_data_generator.classes, y_pred)\n",
    "        plot_confusion_matrix(cm, target_names, images_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # Plot ROC curve per class and print AUC score\n",
    "        print('ROC AUC score:', multiclass_roc_auc_score(images_model_save_path, test_data_generator.classes, y_pred))\n",
    "\n",
    "        # plot learning rate decay\n",
    "        step_decay_schedule.plot(images_model_save_path, numpy.arange(0, epochs))\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_data_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold == True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12a825-29e8-4a20-abfa-5fda2369fb7b",
   "metadata": {},
   "source": [
    "#### **Developer harness test for Stratified K-Fold Cross Validation for images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ae0e788-93d1-4e85-b03d-3d11b78619d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing model\n",
    "#inputs = Input(shape=(128, 128, 3))\n",
    "#x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "#x = MaxPooling2D(pool_size=(8, 8))(x)\n",
    "#x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(8, 8))(x)\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(64)(x)\n",
    "#outputs = Dense(5, activation=\"softmax\")(x)\n",
    "#model = Model(inputs, outputs)\n",
    "\n",
    "# train, validate and test final model\n",
    "#images_kfold_validation_model(model_name=\"testing-1\", n_splits=6, test_size=0.10,\n",
    "#                        shuffle=True, model=model, learning_rate=0.001,\n",
    "#                        decay=0.001, target_size=(128, 128), epochs=10,\n",
    "#                        batch_size=16, one_fold=True, resample_data=150, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be80bb68-58a9-40a5-8772-1d3057328661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing model\n",
    "#layers = [\n",
    "#    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "#    MaxPooling2D(pool_size=(8, 8)),\n",
    "#    Conv2D(64, (3, 3), activation='relu'),\n",
    "#    MaxPooling2D(pool_size=(8, 8)),\n",
    "#    Flatten(),\n",
    "#    Dense(64),\n",
    "#    Activation('relu'),\n",
    "#    Dense(5, activation='softmax')\n",
    "#]\n",
    "\n",
    "# train, validate and test final model\n",
    "#images_kfold_validation_layers(model_name=\"testing-2\", n_splits=6, test_size=0.10,\n",
    "#                        shuffle=True, layers=layers, learning_rate=0.001,\n",
    "#                        decay=0.001, target_size=(128, 128), epochs=10,\n",
    "#                        batch_size=16, one_fold=True, resample_data=150, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14c454b0-1b77-41ec-aafe-5421a2c8de7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment model layers\n",
    "#layers = [\n",
    "#    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "#    MaxPooling2D(pool_size=(2, 2)),\n",
    "#    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "#    MaxPooling2D(pool_size=(2, 2)),\n",
    "#    Flatten(),\n",
    "#    Dense(32, activation='relu'),\n",
    "#    Dense(5, activation='softmax')\n",
    "#]\n",
    "\n",
    "# train, validate and test\n",
    "#images_kfold_validation_layers(model_name=\"testing-3\", n_splits=6, test_size=0.01,\n",
    "#                        shuffle=False, layers=layers, learning_rate=0.001,\n",
    "#                        decay=1e-6, target_size=(128, 128), epochs=10,\n",
    "#                        batch_size=32, one_fold=True, resample_data=200, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b66050c-1265-403a-9938-dc76e5b9bf16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing model\n",
    "#inputs = Input(shape=(128, 128, 3))\n",
    "#x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
    "#x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "#x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "#x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(32)(x)\n",
    "#outputs = Dense(5, activation=\"softmax\")(x)\n",
    "#model = Model(inputs, outputs)\n",
    "#\n",
    "# train, validate and test final model\n",
    "#images_kfold_validation_model(model_name=\"testing-4\", n_splits=6, test_size=0.01,\n",
    "#                        shuffle=True, model=model, learning_rate=0.001,\n",
    "#                        decay=0.001, target_size=(128, 128), epochs=10,\n",
    "#                        batch_size=16, one_fold=True, resample_data=300, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7801313-971f-45cb-a103-997c58b84363",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Stratified K-Fold Cross Validation for pointclouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0c3c5b7-3d60-4810-b18c-0ed075d6ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom learning rate scheduler for pointnet\n",
    "def pointnet_lr_scheduler(epoch, lr):\n",
    "    if (epoch>0) and (epoch%20)==0:\n",
    "        new_lr = (epoch/2)*(lr/epoch)\n",
    "        return new_lr\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4846391a-05d9-4fac-9e64-5e4b4be1ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointclouds_kfold_validation_model(model_name, n_splits, test_size, shuffle, model, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global pointclouds_data\n",
    "    global pointclouds_X\n",
    "    global pointclouds_Y\n",
    "    if resample_data > 0:\n",
    "        pointclouds_data = pointclouds_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        pointclouds_X = pointclouds_data[['filename']]\n",
    "        pointclouds_Y = pointclouds_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "    \n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "    \n",
    "    # split train and test dataset\n",
    "    pointclouds_train, pointclouds_test = train_test_split(pointclouds_data, test_size=test_size, stratify=pointclouds_Y)\n",
    "    pointclouds_train_X = pointclouds_train[['filename']]\n",
    "    pointclouds_train_Y = pointclouds_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # test data arrays\n",
    "    test_pointclouds = []\n",
    "    test_labels = []\n",
    "    test_string_labels = {}\n",
    "\n",
    "    # test split ready\n",
    "    for index, test_data_row in pointclouds_test.iterrows():\n",
    "        test_sampled_mesh = trimesh.load(test_data_row['filename'], force='mesh').sample(target_size)\n",
    "        if augment == True:\n",
    "            test_normalized_mesh = test_sampled_mesh - numpy.mean(test_sampled_mesh, axis=0) \n",
    "            test_normalized_mesh /= numpy.max(numpy.linalg.norm(test_normalized_mesh, axis=1))\n",
    "            test_pointclouds.append(test_normalized_mesh)\n",
    "        else:\n",
    "            test_pointclouds.append(test_sampled_mesh)\n",
    "        test_labels.append(class_labels_dict[test_data_row['class_label']])\n",
    "        test_string_labels[index] = test_data_row['class_label']\n",
    "    \n",
    "    # convert to numpy array\n",
    "    test_pointclouds = numpy.array(test_pointclouds)\n",
    "    test_labels = numpy.array(test_labels)\n",
    "\n",
    "    # create test tf.data.Dataset\n",
    "    test_dataset = tensorflow.data.Dataset.from_tensor_slices((test_pointclouds, test_labels))\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    print(\"Found \" + str(len(pointclouds_test)) + \" validated pointcloud filenames belonging to \" + str(len(pointclouds_test['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "\n",
    "    # generate train and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(pointclouds_train_X, pointclouds_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(pointclouds_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(pointclouds_models_save_dir, model_name))\n",
    "        pointclouds_model_save_path = os.path.join(pointclouds_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # train and validation data arrays\n",
    "        train_pointclouds = []\n",
    "        train_labels = []\n",
    "        train_string_labels = {}\n",
    "        validation_pointclouds = []\n",
    "        validation_labels = []\n",
    "        validation_string_labels = {}\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = pointclouds_train.iloc[train_index]\n",
    "        validation_data = pointclouds_train.iloc[validation_index]\n",
    "\n",
    "        # load training samples meshes\n",
    "        for index, training_data_row in training_data.iterrows():\n",
    "            train_sampled_mesh = trimesh.load(training_data_row['filename'], force='mesh').sample(target_size)\n",
    "            if augment == True:\n",
    "                # nroamlize training sample\n",
    "                train_normalized_mesh = train_sampled_mesh - numpy.mean(train_sampled_mesh, axis=0)\n",
    "                train_normalized_mesh /= numpy.max(numpy.linalg.norm(train_normalized_mesh, axis=1))\n",
    "\n",
    "                for i in range(4):\n",
    "                    # augment: apply random rotations\n",
    "                    theta = random.random() * 2. * math.pi\n",
    "                    rot_matrix = numpy.array([[ math.cos(theta), -math.sin(theta),    0],\n",
    "                                              [ math.sin(theta),  math.cos(theta),    0],\n",
    "                                              [0,                             0,      1]])\n",
    "                    rotated_mesh = rot_matrix.dot(train_normalized_mesh.T).T\n",
    "\n",
    "                    # augment: add some noise\n",
    "                    noise = numpy.random.normal(0, 0.02, (train_normalized_mesh.shape))\n",
    "                    noisy_rotated_mesh = rotated_mesh + noise\n",
    "                    train_pointclouds.append(noisy_rotated_mesh)\n",
    "                    \n",
    "                    # append label for augmented sample\n",
    "                    train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "\n",
    "                train_pointclouds.append(train_normalized_mesh)\n",
    "            else:\n",
    "                train_pointclouds.append(train_sampled_mesh)\n",
    "            train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "            train_string_labels[index] = training_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        train_pointclouds = numpy.array(train_pointclouds)\n",
    "        train_labels = numpy.array(train_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        train_dataset = tensorflow.data.Dataset.from_tensor_slices((train_pointclouds, train_labels))\n",
    "        train_dataset = train_dataset.shuffle(len(train_pointclouds)).batch(batch_size)\n",
    "        print(\"Found \" + str(len(training_data)) + \" validated pointcloud filenames belonging to \" + str(len(training_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # load validation samples meshes\n",
    "        for index, validation_data_row in validation_data.iterrows():\n",
    "            validation_sampled_mesh = trimesh.load(validation_data_row['filename'], force='mesh').sample(target_size)\n",
    "            if augment == True:\n",
    "                # normalize validation sample\n",
    "                validation_normalized_mesh = validation_sampled_mesh - numpy.mean(validation_sampled_mesh, axis=0) \n",
    "                validation_normalized_mesh /= numpy.max(numpy.linalg.norm(validation_normalized_mesh, axis=1))\n",
    "                validation_pointclouds.append(validation_normalized_mesh)\n",
    "            else:\n",
    "                validation_pointclouds.append(validation_sampled_mesh)\n",
    "            validation_labels.append(class_labels_dict[validation_data_row['class_label']])\n",
    "            validation_string_labels[index] = validation_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        validation_pointclouds = numpy.array(validation_pointclouds)\n",
    "        validation_labels = numpy.array(validation_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        validation_dataset = tensorflow.data.Dataset.from_tensor_slices((validation_pointclouds, validation_labels))\n",
    "        validation_dataset = validation_dataset.batch(batch_size)\n",
    "        print(\"Found \" + str(len(validation_data)) + \" validated pointcloud filenames belonging to \" + str(len(validation_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(pointclouds_model_save_path, monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "        val_accuracy_early_stopping = tensorflow.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', mode='max', min_delta=0.001, patience=100, verbose=1)\n",
    "        lr_scheduler_callback = tensorflow.keras.callbacks.LearningRateScheduler(pointnet_lr_scheduler)\n",
    "        callbacks_list = [best_model_checkpoint, val_accuracy_early_stopping, lr_scheduler_callback]\n",
    "\n",
    "        # compile model\n",
    "        model.summary()\n",
    "        model.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate, decay=decay), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_dataset)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(pointclouds_model_save_path, history.history[\"loss\"], history.history[\"sparse_categorical_accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model on the test set\n",
    "        model.load_weights(pointclouds_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_dataset, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_dataset)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=-1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_labels, y_pred)\n",
    "        plot_confusion_matrix(cm, class_labels_dict.keys(), pointclouds_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # Plot ROC curve per class and print AUC score\n",
    "        print('ROC AUC score:', multiclass_roc_auc_score(pointclouds_model_save_path, test_labels, y_pred))\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_labels, y_pred, target_names=class_labels_dict.keys()))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold==True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb44fb-2674-41c7-906e-393c4fac6cb0",
   "metadata": {},
   "source": [
    "#### **Auxiliary method to be used when passing layers array instead of the full model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2cca418-942b-403c-9df8-a51ece205b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointclouds_kfold_validation_layers(model_name, n_splits, test_size, shuffle, layers, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global pointclouds_data\n",
    "    global pointclouds_X\n",
    "    global pointclouds_Y\n",
    "    if resample_data > 0:\n",
    "        pointclouds_data = pointclouds_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        pointclouds_X = pointclouds_data[['filename']]\n",
    "        pointclouds_Y = pointclouds_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "    \n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "    \n",
    "    # split train and test dataset\n",
    "    pointclouds_train, pointclouds_test = train_test_split(pointclouds_data, test_size=test_size, stratify=pointclouds_Y)\n",
    "    pointclouds_train_X = pointclouds_train[['filename']]\n",
    "    pointclouds_train_Y = pointclouds_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # test data arrays\n",
    "    test_pointclouds = []\n",
    "    test_labels = []\n",
    "    test_string_labels = {}\n",
    "\n",
    "    # test split ready\n",
    "    for index, test_data_row in pointclouds_test.iterrows():\n",
    "        test_pointclouds.append(trimesh.load(test_data_row['filename'], force='mesh').sample(target_size))\n",
    "        test_labels.append(class_labels_dict[test_data_row['class_label']])\n",
    "        test_string_labels[index] = test_data_row['class_label']\n",
    "    \n",
    "    # convert to numpy array\n",
    "    test_pointclouds = numpy.array(test_pointclouds)\n",
    "    test_labels = numpy.array(test_labels)\n",
    "\n",
    "    # create test tf.data.Dataset\n",
    "    test_dataset = tensorflow.data.Dataset.from_tensor_slices((test_pointclouds, test_labels))\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    print(\"Found \" + str(len(pointclouds_test)) + \" validated pointcloud filenames belonging to \" + str(len(pointclouds_test['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "\n",
    "    # generate train and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(pointclouds_train_X, pointclouds_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(pointclouds_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(pointclouds_models_save_dir, model_name))\n",
    "        pointclouds_model_save_path = os.path.join(pointclouds_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # train and validation data arrays\n",
    "        train_pointclouds = []\n",
    "        train_labels = []\n",
    "        train_string_labels = {}\n",
    "        validation_pointclouds = []\n",
    "        validation_labels = []\n",
    "        validation_string_labels = {}\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = pointclouds_train.iloc[train_index]\n",
    "        validation_data = pointclouds_train.iloc[validation_index]\n",
    "\n",
    "        for index, training_data_row in training_data.iterrows():\n",
    "            train_pointclouds.append(trimesh.load(training_data_row['filename'], force='mesh').sample(target_size))\n",
    "            train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "            train_string_labels[index] = training_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        train_pointclouds = numpy.array(train_pointclouds)\n",
    "        train_labels = numpy.array(train_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        train_dataset = tensorflow.data.Dataset.from_tensor_slices((train_pointclouds, train_labels))\n",
    "        if augment == True:\n",
    "            train_dataset = train_dataset.shuffle(len(train_pointclouds)).map(augment).batch(batch_size)\n",
    "        else:\n",
    "            train_dataset = train_dataset.shuffle(len(train_pointclouds)).batch(batch_size)\n",
    "        print(\"Found \" + str(len(training_data)) + \" validated pointcloud filenames belonging to \" + str(len(training_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        for index, validation_data_row in validation_data.iterrows():\n",
    "            validation_pointclouds.append(trimesh.load(validation_data_row['filename'], force='mesh').sample(target_size))\n",
    "            validation_labels.append(class_labels_dict[validation_data_row['class_label']])\n",
    "            validation_string_labels[index] = validation_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        validation_pointclouds = numpy.array(validation_pointclouds)\n",
    "        validation_labels = numpy.array(validation_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        validation_dataset = tensorflow.data.Dataset.from_tensor_slices((validation_pointclouds, validation_labels))\n",
    "        validation_dataset = validation_dataset.batch(batch_size)\n",
    "        print(\"Found \" + str(len(validation_data)) + \" validated pointcloud filenames belonging to \" + str(len(validation_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(pointclouds_model_save_path, monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "        #val_accuracy_early_stopping = tensorflow.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', mode='max', min_delta=0.001, patience=50, verbose=1)\n",
    "        callbacks_list = [best_model_checkpoint]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model = models.Sequential(name=model_name + \"-\" + str(fold_counter))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.summary()\n",
    "        model.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate, decay=decay), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_dataset)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(pointclouds_model_save_path, history.history[\"loss\"], history.history[\"sparse_categorical_accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model on the test set\n",
    "        model.load_weights(pointclouds_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_dataset, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_dataset)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=-1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_labels, y_pred)\n",
    "        plot_confusion_matrix(cm, class_labels_dict.keys(), pointclouds_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # Plot ROC curve per class and print AUC score\n",
    "        print('ROC AUC score:', multiclass_roc_auc_score(pointclouds_model_save_path, test_labels, y_pred))\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_labels, y_pred, target_names=class_labels_dict.keys()))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold==True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71b26b-3b79-4d16-b172-3460d2dc8141",
   "metadata": {},
   "source": [
    "#### **Developer harness test for Stratified K-Fold Cross Validation for pointclouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10311024-3492-40c1-b7c8-1c4e5ac2d177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing model\n",
    "#inputs = Input(shape=(4096, 3))\n",
    "#x = Conv1D(32, kernel_size=1, activation='relu')(inputs)\n",
    "#x = Conv1D(64, kernel_size=1, activation='relu')(x)\n",
    "#x = Conv1D(128, kernel_size=1, activation='relu')(x)\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(64)(x)\n",
    "#outputs = Dense(5, activation=\"softmax\")(x)\n",
    "#model = Model(inputs, outputs)\n",
    "\n",
    "# train, validate and test\n",
    "#pointclouds_kfold_validation_model(model_name=\"testing-1\", n_splits=6, test_size=0.05,\n",
    "#                        shuffle=True, model=model, learning_rate=0.001,\n",
    "#                        decay=1e-6, target_size=4096, epochs=10,\n",
    "#                        batch_size=32, one_fold=True, resample_data=100, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3156057-6083-4379-908e-ca3eed140352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing model\n",
    "#layers = [\n",
    "#    Conv1D(32, kernel_size=1, activation='relu', input_shape=(4096, 3)),\n",
    "#    Conv1D(64, kernel_size=1, activation='relu'),\n",
    "#    Conv1D(128, kernel_size=1, activation='relu'),\n",
    "#    Flatten(),\n",
    "#    Dense(32, activation='relu'),\n",
    "#    Dense(5, activation='softmax')\n",
    "#]\n",
    "\n",
    "# train, validate and test\n",
    "#pointclouds_kfold_validation_layers(model_name=\"testing-2\", n_splits=6, test_size=0.05,\n",
    "#                        shuffle=True, layers=layers, learning_rate=0.001,\n",
    "#                        decay=1e-6, target_size=4096, epochs=10,\n",
    "#                        batch_size=32, one_fold=True, resample_data=100, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08908b-b584-4f78-b4c2-49d0188b2bb7",
   "metadata": {},
   "source": [
    "#### **For demonstration purposes only:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c6c53f8-38fe-4387-9147-d01161b3694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointclouds_kfold_validation_conv2d(model_name, n_splits, test_size, shuffle, layers, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global pointclouds_data\n",
    "    global pointclouds_X\n",
    "    global pointclouds_Y\n",
    "    if resample_data > 0:\n",
    "        pointclouds_data = pointclouds_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        pointclouds_X = pointclouds_data[['filename']]\n",
    "        pointclouds_Y = pointclouds_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "    \n",
    "    # instanciate image generator without data augmentation\n",
    "    pointclouds_data_generator = ImageDataGenerator()\n",
    "    \n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "    \n",
    "    # split train and test dataset\n",
    "    pointclouds_train, pointclouds_test = train_test_split(pointclouds_data, test_size=test_size, stratify=pointclouds_Y)\n",
    "    pointclouds_train_X = pointclouds_train[['filename']]\n",
    "    pointclouds_train_Y = pointclouds_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # test data arrays\n",
    "    test_pointclouds = []\n",
    "    test_labels = []\n",
    "    test_string_labels = {}\n",
    "\n",
    "    # test split ready\n",
    "    for index, test_data_row in pointclouds_test.iterrows():\n",
    "        loaded_mesh = trimesh.load(test_data_row['filename'], force='mesh').sample(target_size)\n",
    "        loaded_mesh = loaded_mesh.reshape(128, 128, 3)\n",
    "        test_pointclouds.append(loaded_mesh)\n",
    "        test_labels.append(class_labels_dict[test_data_row['class_label']])\n",
    "        test_string_labels[index] = test_data_row['class_label']\n",
    "    \n",
    "    # convert to numpy array\n",
    "    test_pointclouds = numpy.array(test_pointclouds)\n",
    "    test_labels = numpy.array(test_labels)\n",
    "\n",
    "    # create test tf.data.Dataset\n",
    "    test_data_generator = pointclouds_data_generator.flow(test_pointclouds, test_labels, batch_size=batch_size)\n",
    "    print(\"Found \" + str(len(pointclouds_test)) + \" validated pointcloud filenames belonging to \" + str(len(pointclouds_test['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "    # generate train and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(pointclouds_train_X, pointclouds_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(pointclouds_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(pointclouds_models_save_dir, model_name))\n",
    "        pointclouds_model_save_path = os.path.join(pointclouds_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # train and validation data arrays\n",
    "        train_pointclouds = []\n",
    "        train_labels = []\n",
    "        train_string_labels = {}\n",
    "        validation_pointclouds = []\n",
    "        validation_labels = []\n",
    "        validation_string_labels = {}\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = pointclouds_train.iloc[train_index]\n",
    "        validation_data = pointclouds_train.iloc[validation_index]\n",
    "\n",
    "        for index, training_data_row in training_data.iterrows():\n",
    "            loaded_mesh = trimesh.load(training_data_row['filename'], force='mesh').sample(target_size)\n",
    "            loaded_mesh = loaded_mesh.reshape(128, 128, 3)\n",
    "            train_pointclouds.append(loaded_mesh)\n",
    "            train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "            train_string_labels[index] = training_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        train_pointclouds = numpy.array(train_pointclouds)\n",
    "        train_labels = numpy.array(train_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        train_data_generator = pointclouds_data_generator.flow(train_pointclouds, train_labels, batch_size=batch_size)\n",
    "        print(\"Found \" + str(len(training_data)) + \" validated pointcloud filenames belonging to \" + str(len(training_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        for index, validation_data_row in validation_data.iterrows():\n",
    "            loaded_mesh = trimesh.load(validation_data_row['filename'], force='mesh').sample(target_size)\n",
    "            loaded_mesh = loaded_mesh.reshape(128, 128, 3)\n",
    "            validation_pointclouds.append(loaded_mesh)\n",
    "            validation_labels.append(class_labels_dict[validation_data_row['class_label']])\n",
    "            validation_string_labels[index] = validation_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        validation_pointclouds = numpy.array(validation_pointclouds)\n",
    "        validation_labels = numpy.array(validation_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        validation_data_generator = pointclouds_data_generator.flow(validation_pointclouds, validation_labels, batch_size=batch_size)\n",
    "        print(\"Found \" + str(len(validation_data)) + \" validated pointcloud filenames belonging to \" + str(len(validation_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(pointclouds_model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [best_model_checkpoint]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model = models.Sequential(name=model_name + \"-\" + str(fold_counter))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.summary()\n",
    "        model.compile(optimizer=tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate, decay=decay), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_data_generator,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_data_generator)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(pointclouds_model_save_path, history.history[\"loss\"], history.history[\"sparse_categorical_accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model on the test set\n",
    "        model.load_weights(pointclouds_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_data_generator, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_data_generator)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=-1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_labels, y_pred)\n",
    "        plot_confusion_matrix(cm, class_labels_dict.keys(), pointclouds_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_labels, y_pred, target_names=class_labels_dict.keys()))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold==True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39_venv",
   "language": "python",
   "name": "python39_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
