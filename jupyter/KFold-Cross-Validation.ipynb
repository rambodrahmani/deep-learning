{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afeedc12-2bf9-40d4-a9c4-a4bb4399bf5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bded9b7-56fe-476f-93d5-fe0bd14170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import trimesh\n",
    "import logging\n",
    "import itertools\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Conv2D, MaxPooling2D\n",
    "\n",
    "# increase matplotlib plots font size\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# dataset root path\n",
    "DATASET_ROOT = '/run/media/rr/M2/DevOps/jupyter-lab/CIDL/dataset/'\n",
    "\n",
    "# final preprocessed dataset directory path\n",
    "DATASET_PATH = os.path.join(DATASET_ROOT, 'Preprocessed')\n",
    "\n",
    "# directory where to save the best model for each fold\n",
    "saved_models = 'saved_models'\n",
    "images_models_save_dir = os.path.join(saved_models, 'images')\n",
    "pointclouds_models_save_dir = os.path.join(saved_models, 'pointclouds')\n",
    "\n",
    "# needed to create pointclouds dataset\n",
    "class_labels_dict = {'table':0, 'chair':1, 'lamp':2, 'dresser':3, 'sofa':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b543ec-083a-4573-bc2b-39f0eccdfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create saved_models directory if not present\n",
    "if not os.path.isdir(saved_models):\n",
    "    os.mkdir(saved_models)\n",
    "    \n",
    "if not os.path.isdir(images_models_save_dir):\n",
    "    os.mkdir(images_models_save_dir)\n",
    "\n",
    "if not os.path.isdir(pointclouds_models_save_dir):\n",
    "    os.mkdir(pointclouds_models_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f62597b-f41a-48fd-90ad-3904432f1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to get model name\n",
    "def get_model_name(model_name, fold_counter):\n",
    "    return model_name + '-fold-' + str(fold_counter) + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d2d156-539f-4f3f-85bd-acbe9dec8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and acuracy for each epoch\n",
    "def plot_train_loss_accuracy(save_path, train_loss, train_accuracy, val_loss, val_accuracy):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(train_loss,'o-g', label=\"Training Set\")\n",
    "    plt.plot(val_loss,'o-r', label=\"Validation Set\")\n",
    "    plt.title('Training and Validation Sets Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(save_path + '-train-val-loss.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(train_accuracy,'o-g', label=\"Training Set\")\n",
    "    plt.plot(val_accuracy,'o-r', label=\"Validation Set\")\n",
    "    plt.title('Training and Validation Sets Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(save_path + '-train-val-accuracy.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95378c68-caaf-4917-a2ff-66c626da2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function used to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, save_path, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = numpy.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "        cm = numpy.around(cm, decimals=2)\n",
    "        cm[numpy.isnan(cm)] = 0.0\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path + '-confusion-matrix.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d158d42a-301f-4d43-b7b2-7a2cc737ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to augment pointclouds data\n",
    "def augment(points, label):\n",
    "    # jitter points\n",
    "    points += tensorflow.random.uniform(points.shape, -0.005, 0.005, dtype=tensorflow.float64)\n",
    "    # shuffle points\n",
    "    points = tensorflow.random.shuffle(points)\n",
    "    return points, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3186d841-5e1d-4237-8a25-f49cc0f3115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only log critical messages\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290f4fbd-457c-457e-8ab7-df7b8f31b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images csv file\n",
    "images_data = pandas.read_csv(os.path.join(DATASET_PATH, 'images.csv'))\n",
    "\n",
    "# extract images path and class labels\n",
    "images_X = images_data[['filename']]\n",
    "images_Y = images_data[['class_label']]\n",
    "\n",
    "# load pointclouds csv file\n",
    "pointclouds_data = pandas.read_csv(os.path.join(DATASET_PATH, 'pointclouds.csv'))\n",
    "\n",
    "# extract images path and class labels\n",
    "pointclouds_X = pointclouds_data[['filename']]\n",
    "pointclouds_Y = pointclouds_data[['class_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f15531-b289-41a7-8b18-20e12c88676e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Stratified K-Fold Cross Validation for images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27fb91c7-8105-4d85-86c7-c53308967008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_kfold_validation(model_name, n_splits, test_size, shuffle, layers, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global images_data\n",
    "    global images_X\n",
    "    global images_Y\n",
    "    if resample_data > 0:\n",
    "        images_data = images_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        images_X = images_data[['filename']]\n",
    "        images_Y = images_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "\n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "\n",
    "    # split train and test dataset\n",
    "    images_train, images_test = train_test_split(images_data, test_size=test_size, stratify=images_Y)\n",
    "    images_train_X = images_train[['filename']]\n",
    "    images_train_Y = images_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # instanciate image generator with data augmentation if required\n",
    "    if augment == True:\n",
    "        image_data_generator = ImageDataGenerator(rotation_range=40, width_shift_range=0.01,\n",
    "                                                  height_shift_range=0.01, horizontal_flip=True,\n",
    "                                                  shear_range=20.0, fill_mode='nearest', rescale=1./255)\n",
    "    else:\n",
    "        image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # test split\n",
    "    test_data_generator  = image_data_generator.flow_from_dataframe(images_test, directory=None,\n",
    "                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                        class_mode=\"categorical\", shuffle=shuffle,\n",
    "                                                        target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "    # generate training and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(images_train_X, images_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(images_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(images_models_save_dir, model_name))\n",
    "        images_model_save_path = os.path.join(images_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = images_train.iloc[train_index]\n",
    "        validation_data = images_train.iloc[validation_index]\n",
    "        train_data_generator = image_data_generator.flow_from_dataframe(training_data, directory=None,\n",
    "                                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                                        class_mode=\"categorical\", shuffle=shuffle,\n",
    "                                                                        target_size=target_size, batch_size=batch_size)\n",
    "        validation_data_generator = image_data_generator.flow_from_dataframe(validation_data, directory=None,\n",
    "                                                                        x_col=\"filename\", y_col=\"class_label\",\n",
    "                                                                        class_mode=\"categorical\", shuffle=shuffle,\n",
    "                                                                        target_size=target_size, batch_size=batch_size)\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(images_model_save_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "        #val_accuracy_early_stopping = tensorflow.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.01, patience=30, verbose=1)\n",
    "        callbacks_list = [best_model_checkpoint]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model = models.Sequential(name=model_name + \"-\" + str(fold_counter))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.summary()\n",
    "        model.compile(tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate, decay=decay), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_data_generator,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_data_generator)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(images_model_save_path, history.history[\"loss\"], history.history[\"accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model\n",
    "        model.load_weights(images_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_data_generator, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # --- Report ---\n",
    "        target_names = []\n",
    "        for key in test_data_generator.class_indices:\n",
    "            target_names.append(key)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_data_generator)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_data_generator.classes, y_pred)\n",
    "        plot_confusion_matrix(cm, target_names, images_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_data_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold == True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12a825-29e8-4a20-abfa-5fda2369fb7b",
   "metadata": {},
   "source": [
    "#### **Developer harness test for Stratified K-Fold Cross Validation for images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be80bb68-58a9-40a5-8772-1d3057328661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "#layers = [\n",
    "#    Conv2D(128, (3, 3), input_shape=(128, 128, 3)),\n",
    "#    Activation('relu'),\n",
    "#    MaxPooling2D(pool_size=(8, 8)),\n",
    "#    Conv2D(256, (3, 3)),\n",
    "#    Activation('relu'),\n",
    "#    MaxPooling2D(pool_size=(8, 8)),\n",
    "#    Flatten(),\n",
    "#    Dense(64),\n",
    "#    Activation('relu'),\n",
    "#    Dense(5, activation='softmax')\n",
    "#]\n",
    "\n",
    "# train, validate and test\n",
    "#images_kfold_validation(\"images-testing\", 6, 0.20, True, layers, 0.001, 1e-6, (128,128), 50, 32, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7801313-971f-45cb-a103-997c58b84363",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Stratified K-Fold Cross Validation for pointclouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4846391a-05d9-4fac-9e64-5e4b4be1ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointclouds_kfold_validation(model_name, n_splits, test_size, shuffle, layers, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global pointclouds_data\n",
    "    global pointclouds_X\n",
    "    global pointclouds_Y\n",
    "    if resample_data > 0:\n",
    "        pointclouds_data = pointclouds_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        pointclouds_X = pointclouds_data[['filename']]\n",
    "        pointclouds_Y = pointclouds_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "    \n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "    \n",
    "    # split train and test dataset\n",
    "    pointclouds_train, pointclouds_test = train_test_split(pointclouds_data, test_size=test_size, stratify=pointclouds_Y)\n",
    "    pointclouds_train_X = pointclouds_train[['filename']]\n",
    "    pointclouds_train_Y = pointclouds_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # test data arrays\n",
    "    test_pointclouds = []\n",
    "    test_labels = []\n",
    "    test_string_labels = {}\n",
    "\n",
    "    # test split ready\n",
    "    for index, test_data_row in pointclouds_test.iterrows():\n",
    "        test_pointclouds.append(trimesh.load(test_data_row['filename'], force='mesh').sample(target_size))\n",
    "        test_labels.append(class_labels_dict[test_data_row['class_label']])\n",
    "        test_string_labels[index] = test_data_row['class_label']\n",
    "    \n",
    "    # convert to numpy array\n",
    "    test_pointclouds = numpy.array(test_pointclouds)\n",
    "    test_labels = numpy.array(test_labels)\n",
    "\n",
    "    # create test tf.data.Dataset\n",
    "    test_dataset = tensorflow.data.Dataset.from_tensor_slices((test_pointclouds, test_labels))\n",
    "    test_dataset = test_dataset.shuffle(len(test_pointclouds)).batch(batch_size)\n",
    "    print(\"Found \" + str(len(pointclouds_test)) + \" validated pointcloud filenames belonging to \" + str(len(pointclouds_test['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "\n",
    "    # generate train and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(pointclouds_train_X, pointclouds_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(pointclouds_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(pointclouds_models_save_dir, model_name))\n",
    "        pointclouds_model_save_path = os.path.join(pointclouds_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # train and validation data arrays\n",
    "        train_pointclouds = []\n",
    "        train_labels = []\n",
    "        train_string_labels = {}\n",
    "        validation_pointclouds = []\n",
    "        validation_labels = []\n",
    "        validation_string_labels = {}\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = pointclouds_train.iloc[train_index]\n",
    "        validation_data = pointclouds_train.iloc[validation_index]\n",
    "\n",
    "        for index, training_data_row in training_data.iterrows():\n",
    "            train_pointclouds.append(trimesh.load(training_data_row['filename'], force='mesh').sample(target_size))\n",
    "            train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "            train_string_labels[index] = training_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        train_pointclouds = numpy.array(train_pointclouds)\n",
    "        train_labels = numpy.array(train_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        train_dataset = tensorflow.data.Dataset.from_tensor_slices((train_pointclouds, train_labels))\n",
    "        if augment == True:\n",
    "            train_dataset = train_dataset.shuffle(len(train_pointclouds)).map(augment).batch(batch_size)\n",
    "        else:\n",
    "            train_dataset = train_dataset.shuffle(len(train_pointclouds)).batch(batch_size)\n",
    "        print(\"Found \" + str(len(training_data)) + \" validated pointcloud filenames belonging to \" + str(len(training_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        for index, validation_data_row in validation_data.iterrows():\n",
    "            validation_pointclouds.append(trimesh.load(validation_data_row['filename'], force='mesh').sample(target_size))\n",
    "            validation_labels.append(class_labels_dict[validation_data_row['class_label']])\n",
    "            validation_string_labels[index] = validation_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        validation_pointclouds = numpy.array(validation_pointclouds)\n",
    "        validation_labels = numpy.array(validation_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        validation_dataset = tensorflow.data.Dataset.from_tensor_slices((validation_pointclouds, validation_labels))\n",
    "        if augment == True:\n",
    "            validation_dataset = validation_dataset.shuffle(len(validation_pointclouds)).map(augment).batch(batch_size)\n",
    "        else:\n",
    "            validation_dataset = validation_dataset.shuffle(len(validation_pointclouds)).batch(batch_size)\n",
    "        print(\"Found \" + str(len(validation_data)) + \" validated pointcloud filenames belonging to \" + str(len(validation_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(pointclouds_model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [best_model_checkpoint]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model = models.Sequential(name=model_name + \"-\" + str(fold_counter))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.summary()\n",
    "        model.compile(optimizer=tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate, decay=decay), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_dataset)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(pointclouds_model_save_path, history.history[\"loss\"], history.history[\"sparse_categorical_accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model on the test set\n",
    "        model.load_weights(pointclouds_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_dataset, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_dataset)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_labels, y_pred)\n",
    "        plot_confusion_matrix(cm, class_labels_dict.keys(), pointclouds_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_labels, y_pred, target_names=class_labels_dict.keys()))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold==True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71b26b-3b79-4d16-b172-3460d2dc8141",
   "metadata": {},
   "source": [
    "#### **Developer harness test for Stratified K-Fold Cross Validation for pointclouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3156057-6083-4379-908e-ca3eed140352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "#layers = [\n",
    "#    Conv1D(32, 3, padding='same', input_shape=(2048, 3)),\n",
    "#    BatchNormalization(momentum=0.0),\n",
    "#    Activation('relu'),\n",
    "#    Conv1D(64, 3, padding='same', input_shape=(2048, 3)),\n",
    "#    BatchNormalization(momentum=0.0),\n",
    "#    Activation('relu'),\n",
    "#    Conv1D(512, 3, padding='same', input_shape=(2048, 3)),\n",
    "#    BatchNormalization(momentum=0.0),\n",
    "#    Activation('relu'),\n",
    "#    GlobalMaxPooling1D(),\n",
    "#    Dense(512),\n",
    "#    BatchNormalization(momentum=0.0),\n",
    "#    Flatten(),\n",
    "#    Dense(1024),\n",
    "#    Activation('relu'),\n",
    "#    Dense(5, activation='softmax')\n",
    "#]\n",
    "\n",
    "# train, validate and test\n",
    "#pointclouds_kfold_validation(\"pointclouds-testing\", 6, 0.20, True, layers, 0.001, 1e-6, 2048, 50, 32, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08908b-b584-4f78-b4c2-49d0188b2bb7",
   "metadata": {},
   "source": [
    "#### **For demonstration purposes only:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c6c53f8-38fe-4387-9147-d01161b3694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointclouds_kfold_validation_conv2d(model_name, n_splits, test_size, shuffle, layers, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global pointclouds_data\n",
    "    global pointclouds_X\n",
    "    global pointclouds_Y\n",
    "    if resample_data > 0:\n",
    "        pointclouds_data = pointclouds_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        pointclouds_X = pointclouds_data[['filename']]\n",
    "        pointclouds_Y = pointclouds_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "    \n",
    "    # instanciate image generator without data augmentation\n",
    "    pointclouds_data_generator = ImageDataGenerator()\n",
    "    \n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "    \n",
    "    # split train and test dataset\n",
    "    pointclouds_train, pointclouds_test = train_test_split(pointclouds_data, test_size=test_size, stratify=pointclouds_Y)\n",
    "    pointclouds_train_X = pointclouds_train[['filename']]\n",
    "    pointclouds_train_Y = pointclouds_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # test data arrays\n",
    "    test_pointclouds = []\n",
    "    test_labels = []\n",
    "    test_string_labels = {}\n",
    "\n",
    "    # test split ready\n",
    "    for index, test_data_row in pointclouds_test.iterrows():\n",
    "        loaded_mesh = trimesh.load(test_data_row['filename'], force='mesh').sample(target_size)\n",
    "        loaded_mesh = loaded_mesh.reshape(128, 128, 3)\n",
    "        test_pointclouds.append(loaded_mesh)\n",
    "        test_labels.append(class_labels_dict[test_data_row['class_label']])\n",
    "        test_string_labels[index] = test_data_row['class_label']\n",
    "    \n",
    "    # convert to numpy array\n",
    "    test_pointclouds = numpy.array(test_pointclouds)\n",
    "    test_labels = numpy.array(test_labels)\n",
    "\n",
    "    # create test tf.data.Dataset\n",
    "    test_data_generator = pointclouds_data_generator.flow(test_pointclouds, test_labels, batch_size=batch_size)\n",
    "    print(\"Found \" + str(len(pointclouds_test)) + \" validated pointcloud filenames belonging to \" + str(len(pointclouds_test['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "    # generate train and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(pointclouds_train_X, pointclouds_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(pointclouds_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(pointclouds_models_save_dir, model_name))\n",
    "        pointclouds_model_save_path = os.path.join(pointclouds_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # train and validation data arrays\n",
    "        train_pointclouds = []\n",
    "        train_labels = []\n",
    "        train_string_labels = {}\n",
    "        validation_pointclouds = []\n",
    "        validation_labels = []\n",
    "        validation_string_labels = {}\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = pointclouds_train.iloc[train_index]\n",
    "        validation_data = pointclouds_train.iloc[validation_index]\n",
    "\n",
    "        for index, training_data_row in training_data.iterrows():\n",
    "            loaded_mesh = trimesh.load(training_data_row['filename'], force='mesh').sample(target_size)\n",
    "            loaded_mesh = loaded_mesh.reshape(128, 128, 3)\n",
    "            train_pointclouds.append(loaded_mesh)\n",
    "            train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "            train_string_labels[index] = training_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        train_pointclouds = numpy.array(train_pointclouds)\n",
    "        train_labels = numpy.array(train_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        train_data_generator = pointclouds_data_generator.flow(train_pointclouds, train_labels, batch_size=batch_size)\n",
    "        print(\"Found \" + str(len(training_data)) + \" validated pointcloud filenames belonging to \" + str(len(training_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        for index, validation_data_row in validation_data.iterrows():\n",
    "            loaded_mesh = trimesh.load(validation_data_row['filename'], force='mesh').sample(target_size)\n",
    "            loaded_mesh = loaded_mesh.reshape(128, 128, 3)\n",
    "            validation_pointclouds.append(loaded_mesh)\n",
    "            validation_labels.append(class_labels_dict[validation_data_row['class_label']])\n",
    "            validation_string_labels[index] = validation_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        validation_pointclouds = numpy.array(validation_pointclouds)\n",
    "        validation_labels = numpy.array(validation_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        validation_data_generator = pointclouds_data_generator.flow(validation_pointclouds, validation_labels, batch_size=batch_size)\n",
    "        print(\"Found \" + str(len(validation_data)) + \" validated pointcloud filenames belonging to \" + str(len(validation_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(pointclouds_model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [best_model_checkpoint]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model = models.Sequential(name=model_name + \"-\" + str(fold_counter))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.summary()\n",
    "        model.compile(optimizer=tensorflow.keras.optimizers.RMSprop(learning_rate=learning_rate, decay=decay), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_data_generator,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_data_generator)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(pointclouds_model_save_path, history.history[\"loss\"], history.history[\"sparse_categorical_accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model on the test set\n",
    "        model.load_weights(pointclouds_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_data_generator, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_data_generator)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_labels, y_pred)\n",
    "        plot_confusion_matrix(cm, class_labels_dict.keys(), pointclouds_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_labels, y_pred, target_names=class_labels_dict.keys()))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold==True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d484253-850b-4d10-af4b-20fce01cfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointclouds_kfold_validation_pointnet(model_name, n_splits, test_size, shuffle, model, learning_rate, decay, target_size, epochs, batch_size, one_fold=True, resample_data=0, augment=False):\n",
    "    global pointclouds_data\n",
    "    global pointclouds_X\n",
    "    global pointclouds_Y\n",
    "    if resample_data > 0:\n",
    "        pointclouds_data = pointclouds_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data)))\n",
    "        pointclouds_X = pointclouds_data[['filename']]\n",
    "        pointclouds_Y = pointclouds_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "    \n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "    \n",
    "    # split train and test dataset\n",
    "    pointclouds_train, pointclouds_test = train_test_split(pointclouds_data, test_size=test_size, stratify=pointclouds_Y)\n",
    "    pointclouds_train_X = pointclouds_train[['filename']]\n",
    "    pointclouds_train_Y = pointclouds_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    # test data arrays\n",
    "    test_pointclouds = []\n",
    "    test_labels = []\n",
    "    test_string_labels = {}\n",
    "\n",
    "    # test split ready\n",
    "    for index, test_data_row in pointclouds_test.iterrows():\n",
    "        test_pointclouds.append(trimesh.load(test_data_row['filename'], force='mesh').sample(target_size))\n",
    "        test_labels.append(class_labels_dict[test_data_row['class_label']])\n",
    "        test_string_labels[index] = test_data_row['class_label']\n",
    "    \n",
    "    # convert to numpy array\n",
    "    test_pointclouds = numpy.array(test_pointclouds)\n",
    "    test_labels = numpy.array(test_labels)\n",
    "\n",
    "    # create test tf.data.Dataset\n",
    "    test_dataset = tensorflow.data.Dataset.from_tensor_slices((test_pointclouds, test_labels))\n",
    "    test_dataset = test_dataset.shuffle(len(test_pointclouds)).batch(batch_size)\n",
    "    print(\"Found \" + str(len(pointclouds_test)) + \" validated pointcloud filenames belonging to \" + str(len(pointclouds_test['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "\n",
    "    # generate train and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(pointclouds_train_X, pointclouds_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(pointclouds_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(pointclouds_models_save_dir, model_name))\n",
    "        pointclouds_model_save_path = os.path.join(pointclouds_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # train and validation data arrays\n",
    "        train_pointclouds = []\n",
    "        train_labels = []\n",
    "        train_string_labels = {}\n",
    "        validation_pointclouds = []\n",
    "        validation_labels = []\n",
    "        validation_string_labels = {}\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = pointclouds_train.iloc[train_index]\n",
    "        validation_data = pointclouds_train.iloc[validation_index]\n",
    "\n",
    "        for index, training_data_row in training_data.iterrows():\n",
    "            train_pointclouds.append(trimesh.load(training_data_row['filename'], force='mesh').sample(target_size))\n",
    "            train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "            train_string_labels[index] = training_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        train_pointclouds = numpy.array(train_pointclouds)\n",
    "        train_labels = numpy.array(train_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        train_dataset = tensorflow.data.Dataset.from_tensor_slices((train_pointclouds, train_labels))\n",
    "        if augment == True:\n",
    "            train_dataset = train_dataset.shuffle(len(train_pointclouds)).map(augment).batch(batch_size)\n",
    "        else:\n",
    "            train_dataset = train_dataset.shuffle(len(train_pointclouds)).batch(batch_size)\n",
    "        print(\"Found \" + str(len(training_data)) + \" validated pointcloud filenames belonging to \" + str(len(training_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        for index, validation_data_row in validation_data.iterrows():\n",
    "            validation_pointclouds.append(trimesh.load(validation_data_row['filename'], force='mesh').sample(target_size))\n",
    "            validation_labels.append(class_labels_dict[validation_data_row['class_label']])\n",
    "            validation_string_labels[index] = validation_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        validation_pointclouds = numpy.array(validation_pointclouds)\n",
    "        validation_labels = numpy.array(validation_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        validation_dataset = tensorflow.data.Dataset.from_tensor_slices((validation_pointclouds, validation_labels))\n",
    "        if augment == True:\n",
    "            validation_dataset = validation_dataset.shuffle(len(validation_pointclouds)).map(augment).batch(batch_size)\n",
    "        else:\n",
    "            validation_dataset = validation_dataset.shuffle(len(validation_pointclouds)).batch(batch_size)\n",
    "        print(\"Found \" + str(len(validation_data)) + \" validated pointcloud filenames belonging to \" + str(len(validation_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(pointclouds_model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [best_model_checkpoint]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model.summary()\n",
    "        model.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learning_rate, decay=decay), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "        # train model\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_dataset)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(pointclouds_model_save_path, history.history[\"loss\"], history.history[\"sparse_categorical_accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model on the test set\n",
    "        model.load_weights(pointclouds_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_dataset, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_dataset)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_labels, y_pred)\n",
    "        plot_confusion_matrix(cm, class_labels_dict.keys(), pointclouds_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_labels, y_pred, target_names=class_labels_dict.keys()))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        print(\"\\n-------- TERMINATED FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # if one fold execution was requested, terminate here\n",
    "        if one_fold==True:\n",
    "            return;\n",
    "        else:\n",
    "            fold_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39_venv",
   "language": "python",
   "name": "python39_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
