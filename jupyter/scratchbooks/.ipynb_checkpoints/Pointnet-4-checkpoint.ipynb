{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afeedc12-2bf9-40d4-a9c4-a4bb4399bf5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **POINTNET WITH K-FOLD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bded9b7-56fe-476f-93d5-fe0bd14170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import trimesh\n",
    "import logging\n",
    "import itertools\n",
    "import tensorflow\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models, layers, regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Conv2D, MaxPooling2D\n",
    "\n",
    "# increase matplotlib plots font size\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# The below is necessary for reproducible results of certain Python hash-based operations.\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "numpy.random.seed(31)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "random.seed(14)\n",
    "\n",
    "# The below tf.random.set_seed will make random number generation in TensorFlow have a well-defined initial state.\n",
    "tensorflow.random.set_seed(1234)\n",
    "\n",
    "# dataset root path\n",
    "DATASET_ROOT = '/run/media/rr/M2/DevOps/jupyter-lab/CIDL/dataset/'\n",
    "\n",
    "# final preprocessed dataset directory path\n",
    "DATASET_PATH = os.path.join(DATASET_ROOT, 'Preprocessed')\n",
    "\n",
    "# directory where to save the best model for each fold\n",
    "images_models_save_dir = 'saved_models/images'\n",
    "pointclouds_models_save_dir = 'saved_models/pointclouds'\n",
    "\n",
    "# needed to create pointclouds dataset\n",
    "class_labels_dict = {'table':0, 'chair':1, 'lamp':2, 'dresser':3, 'sofa':4}\n",
    "\n",
    "# utility function\n",
    "def get_model_name(model_name, fold_counter):\n",
    "    return model_name + '-fold-' + str(fold_counter) + '.h5'\n",
    "\n",
    "# utility function to augment pointclouds data\n",
    "def augment(points, label):\n",
    "    # jitter points\n",
    "    points += tensorflow.random.uniform(points.shape, -0.005, 0.005, dtype=tensorflow.float64)\n",
    "    # shuffle points\n",
    "    points = tensorflow.random.shuffle(points)\n",
    "    return points, label\n",
    "\n",
    "# plot loss and acuracy for each epoch\n",
    "def plot_train_loss_accuracy(save_path, train_loss, train_accuracy, val_loss, val_accuracy):\n",
    "    #print('Training Set Loss: ', train_loss)\n",
    "    #print('Validation Set Loss: ', val_loss)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(train_loss,'o-g', label=\"Training Set\")\n",
    "    plt.plot(val_loss,'o-r', label=\"Validation Set\")\n",
    "    plt.title('Training and Validation Sets Loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(save_path + '-train-val-loss.jpg', dpi=200)\n",
    "    plt.show()\n",
    "    #print('Training Set Accuracy: ', train_accuracy)\n",
    "    #print('Validation Set Accuracy: ', val_accuracy)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(train_accuracy,'o-g', label=\"Training Set\")\n",
    "    plt.plot(val_accuracy,'o-r', label=\"Validation Set\")\n",
    "    plt.title('Training and Validation Sets Accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(save_path + '-train-val-accuracy.jpg', dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, save_path, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = numpy.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, numpy.newaxis]\n",
    "        cm = numpy.around(cm, decimals=2)\n",
    "        cm[numpy.isnan(cm)] = 0.0\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(save_path + '-confusion-matrix.jpg', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3186d841-5e1d-4237-8a25-f49cc0f3115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only log critical messages\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290f4fbd-457c-457e-8ab7-df7b8f31b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images csv file\n",
    "images_data = pandas.read_csv(os.path.join(DATASET_PATH, 'images.csv'))\n",
    "\n",
    "# extract images path and class labels\n",
    "images_X = images_data[['filename']]\n",
    "images_Y = images_data[['class_label']]\n",
    "\n",
    "# load pointclouds csv file\n",
    "pointclouds_data = pandas.read_csv(os.path.join(DATASET_PATH, 'pointclouds.csv'))\n",
    "\n",
    "# extract images path and class labels\n",
    "pointclouds_X = pointclouds_data[['filename']]\n",
    "pointclouds_Y = pointclouds_data[['class_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7801313-971f-45cb-a103-997c58b84363",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Stratified K-Fold Cross Validation for pointclouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4846391a-05d9-4fac-9e64-5e4b4be1ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointclouds_kfold_validation(model_name, n_splits, test_size, shuffle, layers, learning_rate, decay, target_size, epochs, batch_size, resample_data=0):\n",
    "    global pointclouds_data\n",
    "    global pointclouds_X\n",
    "    global pointclouds_Y\n",
    "    if resample_data > 0:\n",
    "        pointclouds_data = pointclouds_data.groupby('class_label', group_keys=False).apply(lambda x: x.sample(min(len(x), resample_data), random_state=42))\n",
    "        pointclouds_X = pointclouds_data[['filename']]\n",
    "        pointclouds_Y = pointclouds_data[['class_label']]\n",
    "\n",
    "    # fold counter\n",
    "    fold_counter = 1\n",
    "    \n",
    "    # arrays to store test set loss and accuracy scores for each fold\n",
    "    TEST_LOSS = []\n",
    "    TEST_ACCURACY = []\n",
    "    \n",
    "    # split train and test dataset\n",
    "    pointclouds_train, pointclouds_test = train_test_split(pointclouds_data, test_size=test_size, stratify=pointclouds_Y, random_state=42)\n",
    "    pointclouds_train_X = pointclouds_train[['filename']]\n",
    "    pointclouds_train_Y = pointclouds_train[['class_label']]\n",
    "\n",
    "    # define stratified k fold cross validation parameters\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, random_state=7, shuffle=shuffle)\n",
    "\n",
    "    # test data arrays\n",
    "    test_pointclouds = []\n",
    "    test_labels = []\n",
    "    test_string_labels = {}\n",
    "\n",
    "    # test split ready\n",
    "    for index, test_data_row in pointclouds_test.iterrows():\n",
    "        test_pointclouds.append(trimesh.load(test_data_row['filename'], force='mesh').sample(target_size))\n",
    "        test_labels.append(class_labels_dict[test_data_row['class_label']])\n",
    "        test_string_labels[index] = test_data_row['class_label']\n",
    "    \n",
    "    # convert to numpy array\n",
    "    test_pointclouds = numpy.array(test_pointclouds)\n",
    "    test_labels = numpy.array(test_labels)\n",
    "    \n",
    "    # debugging\n",
    "    print(len(test_pointclouds))\n",
    "    print(len(test_labels))\n",
    "    print(test_pointclouds)\n",
    "    print(test_labels)\n",
    "\n",
    "    # create test tf.data.Dataset\n",
    "    test_dataset = tensorflow.data.Dataset.from_tensor_slices((test_pointclouds, test_labels))\n",
    "    test_dataset = test_dataset.shuffle(len(test_pointclouds)).batch(batch_size)\n",
    "    print(\"Found \" + str(len(pointclouds_test)) + \" validated pointcloud filenames belonging to \" + str(len(pointclouds_test['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "\n",
    "    # generate train and validation folds\n",
    "    for train_index, validation_index in stratified_kfold.split(pointclouds_train_X, pointclouds_train_Y):\n",
    "        print(\"\\n-------- STARTING FOLD: \" + str(fold_counter) + \" --------\")\n",
    "\n",
    "        # best model save path\n",
    "        if not os.path.isdir(os.path.join(pointclouds_models_save_dir, model_name)):\n",
    "            os.mkdir(os.path.join(pointclouds_models_save_dir, model_name))\n",
    "        pointclouds_model_save_path = os.path.join(pointclouds_models_save_dir, model_name, get_model_name(model_name, fold_counter))\n",
    "\n",
    "        # train and validation data arrays\n",
    "        train_pointclouds = []\n",
    "        train_labels = []\n",
    "        train_string_labels = {}\n",
    "        validation_pointclouds = []\n",
    "        validation_labels = []\n",
    "        validation_string_labels = {}\n",
    "\n",
    "        # training and test folds indices\n",
    "        training_data = pointclouds_train.iloc[train_index]\n",
    "        validation_data = pointclouds_train.iloc[validation_index]\n",
    "\n",
    "        for index, training_data_row in training_data.iterrows():\n",
    "            train_pointclouds.append(trimesh.load(training_data_row['filename'], force='mesh').sample(target_size))\n",
    "            train_labels.append(class_labels_dict[training_data_row['class_label']])\n",
    "            train_string_labels[index] = training_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        train_pointclouds = numpy.array(train_pointclouds)\n",
    "        train_labels = numpy.array(train_labels)\n",
    "    \n",
    "        # debugging\n",
    "        print(len(train_pointclouds))\n",
    "        print(len(train_labels))\n",
    "        print(train_pointclouds)\n",
    "        print(train_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        train_dataset = tensorflow.data.Dataset.from_tensor_slices((train_pointclouds, train_labels))\n",
    "        train_dataset = train_dataset.shuffle(len(train_pointclouds)).map(augment).batch(batch_size)\n",
    "        print(\"Found \" + str(len(training_data)) + \" validated pointcloud filenames belonging to \" + str(len(training_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        for index, validation_data_row in validation_data.iterrows():\n",
    "            validation_pointclouds.append(trimesh.load(validation_data_row['filename'], force='mesh').sample(target_size))\n",
    "            validation_labels.append(class_labels_dict[validation_data_row['class_label']])\n",
    "            validation_string_labels[index] = validation_data_row['class_label']\n",
    "\n",
    "        # convert to numpy array\n",
    "        validation_pointclouds = numpy.array(validation_pointclouds)\n",
    "        validation_labels = numpy.array(validation_labels)\n",
    "    \n",
    "        # debugging\n",
    "        print(len(validation_pointclouds))\n",
    "        print(len(validation_labels))\n",
    "        print(validation_pointclouds)\n",
    "        print(validation_labels)\n",
    "\n",
    "        # create train tf.data.Dataset\n",
    "        validation_dataset = tensorflow.data.Dataset.from_tensor_slices((validation_pointclouds, validation_labels))\n",
    "        validation_dataset = validation_dataset.shuffle(len(validation_pointclouds)).map(augment).batch(batch_size)\n",
    "        print(\"Found \" + str(len(validation_data)) + \" validated pointcloud filenames belonging to \" + str(len(validation_data['class_label'].unique())) + \" classes.\")\n",
    "\n",
    "        # create best model checkpoint\n",
    "        best_model_checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(pointclouds_model_save_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [best_model_checkpoint]\n",
    "\n",
    "        # define model to be trained and tested\n",
    "        model = models.Sequential(name=model_name + \"-\" + str(fold_counter))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            metrics=[\"sparse_categorical_accuracy\"],\n",
    "        )\n",
    "\n",
    "        history = model.fit(train_dataset, epochs=50, validation_data=validation_dataset)\n",
    "\n",
    "        # plot loss and acuracy for each training/validation fold\n",
    "        plot_train_loss_accuracy(pointclouds_model_save_path, history.history[\"loss\"], history.history[\"sparse_categorical_accuracy\"],\n",
    "                                 history.history[\"val_loss\"], history.history[\"val_sparse_categorical_accuracy\"])\n",
    "\n",
    "        # LOAD BEST MODEL to evaluate the performance of the model on the test set\n",
    "        model.load_weights(pointclouds_model_save_path)\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_dataset, batch_size=batch_size)\n",
    "        print(\"Best model Test Loss: \" + str(test_loss))\n",
    "        print(\"Best model Test Accuracy: \" + str(test_accuracy))\n",
    "        \n",
    "        # store test set loss and accuracy scores for each fold\n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACCURACY.append(test_accuracy)\n",
    "\n",
    "        # Confution Matrix \n",
    "        Y_pred = model.predict(test_dataset)\n",
    "        y_pred = numpy.argmax(Y_pred, axis=1)\n",
    "        print('Confusion Matrix')\n",
    "        cm = confusion_matrix(test_labels, y_pred)\n",
    "        plot_confusion_matrix(cm, class_labels_dict.keys(), pointclouds_model_save_path, title='Confusion Matrix')\n",
    "\n",
    "        # print Classification Report\n",
    "        print('Classification Report')\n",
    "        print(classification_report(test_labels, y_pred, target_names=class_labels_dict.keys()))\n",
    "\n",
    "        # clean up before next fold\n",
    "        del model\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "        fold_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71b26b-3b79-4d16-b172-3460d2dc8141",
   "metadata": {},
   "source": [
    "#### **Developer harness test for Stratified K-Fold Cross Validation for pointclouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3156057-6083-4379-908e-ca3eed140352",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GlobalMaxPooling1D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     Conv1D(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m      4\u001b[0m     BatchNormalization(momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m      5\u001b[0m     Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m     Conv1D(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m      7\u001b[0m     BatchNormalization(momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m      8\u001b[0m     Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      9\u001b[0m     Conv1D(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m     10\u001b[0m     BatchNormalization(momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m     11\u001b[0m     Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mGlobalMaxPooling1D\u001b[49m(),\n\u001b[1;32m     13\u001b[0m     Dense(\u001b[38;5;241m512\u001b[39m),\n\u001b[1;32m     14\u001b[0m     BatchNormalization(momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m),\n\u001b[1;32m     15\u001b[0m     Flatten(),\n\u001b[1;32m     16\u001b[0m     Dense(\u001b[38;5;241m1024\u001b[39m),\n\u001b[1;32m     17\u001b[0m     Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     18\u001b[0m     Dense(\u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train, validate and test\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pointclouds_kfold_validation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0.20\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, layers, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GlobalMaxPooling1D' is not defined"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "layers = [\n",
    "    Conv1D(32, 3, padding='same', input_shape=(2048, 3)),\n",
    "    BatchNormalization(momentum=0.0),\n",
    "    Activation('relu'),\n",
    "    Conv1D(64, 3, padding='same', input_shape=(2048, 3)),\n",
    "    BatchNormalization(momentum=0.0),\n",
    "    Activation('relu'),\n",
    "    Conv1D(512, 3, padding='same', input_shape=(2048, 3)),\n",
    "    BatchNormalization(momentum=0.0),\n",
    "    Activation('relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(512),\n",
    "    BatchNormalization(momentum=0.0),\n",
    "    Flatten(),\n",
    "    Dense(1024),\n",
    "    Activation('relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "]\n",
    "\n",
    "# train, validate and test\n",
    "pointclouds_kfold_validation(\"testing\", 3, 0.20, True, layers, 0.001, 1e-6, 2048, 50, 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39_venv",
   "language": "python",
   "name": "python39_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
